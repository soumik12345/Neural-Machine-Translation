{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create_TFRecords.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFZbZFrU42-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "066c7cf2-b4f1-42d5-b2d3-70263d7d0642"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iZykToFwEJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(tfds_address, disable_progress_bar=False):\n",
        "    if disable_progress_bar:\n",
        "        tfds.disable_progress_bar()\n",
        "    data, metadata = tfds.load(tfds_address, with_info=True, as_supervised=True)\n",
        "    train_data, val_data = data['train'], data['validation']\n",
        "    return train_data, val_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv1icbvZwJZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokenizers(dataset, approx_vocab_size=2 ** 13):\n",
        "    source_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "        (i.numpy() for i, j in dataset),\n",
        "        target_vocab_size=approx_vocab_size\n",
        "    )\n",
        "    target_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "        (j.numpy() for i, j in dataset),\n",
        "        target_vocab_size=approx_vocab_size\n",
        "    )\n",
        "    return source_tokenizer, target_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcjqgYOIwMi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader:\n",
        "\n",
        "    def __init__(self, source_tokenizer, target_tokenize, max_limit=40):\n",
        "        self.source_tokenizer = source_tokenizer\n",
        "        self.target_tokenizer = target_tokenizer\n",
        "        self.max_limit = max_limit\n",
        "\n",
        "    def preprocess(self, language_1, language_2):\n",
        "        language_1 = [\n",
        "            self.source_tokenizer.vocab_size\n",
        "        ] + self.source_tokenizer.encode(\n",
        "            language_1.numpy()\n",
        "        ) + [\n",
        "             self.source_tokenizer.vocab_size + 1\n",
        "        ]\n",
        "        language_2 = [\n",
        "            self.target_tokenizer.vocab_size\n",
        "        ] + self.target_tokenizer.encode(\n",
        "            language_2.numpy()\n",
        "        ) + [\n",
        "            self.target_tokenizer.vocab_size + 1\n",
        "        ]\n",
        "        return language_1, language_2\n",
        "\n",
        "    def map_function(self, language_1, language_2):\n",
        "        language_1, language_2 = tf.py_function(\n",
        "            self.preprocess,\n",
        "            [language_1, language_2],\n",
        "            [tf.int64, tf.int64]\n",
        "        )\n",
        "        language_1.set_shape([None])\n",
        "        language_2.set_shape([None])\n",
        "        return language_1, language_2\n",
        "\n",
        "    def filter_max_length(self, x, y):\n",
        "        return tf.logical_and(\n",
        "            tf.size(x) <= self.max_limit,\n",
        "            tf.size(y) <= self.max_limit\n",
        "        )\n",
        "\n",
        "    def get_dataset(self, dataset, buffer_size, batch_size):\n",
        "        tf_dataset = dataset.map(self.map_function)\n",
        "        tf_dataset = tf_dataset.filter(self.filter_max_length)\n",
        "        tf_dataset = tf_dataset.cache()\n",
        "        tf_dataset = tf_dataset.shuffle(buffer_size)\n",
        "        tf_dataset = tf_dataset.padded_batch(batch_size, padded_shapes=([None],[None]))\n",
        "        tf_dataset = tf_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return tf_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLVq3FtawOen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "735a2f8e-841d-4a96-bf4c-927a1551828b"
      },
      "source": [
        "train_data, val_data = download_dataset('ted_hrlr_translate/pt_to_en', disable_progress_bar=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset ted_hrlr_translate (124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJPOJHN/ted_hrlr_translate-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJPOJHN/ted_hrlr_translate-validation.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJPOJHN/ted_hrlr_translate-test.tfrecord\n",
            "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V36Fjzu3xdGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_tokenizer, target_tokenizer = get_tokenizers(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEXDfDCqxr5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "713086e1-110a-4a80-b9d3-51500bd8d350"
      },
      "source": [
        "sample_string = 'Downy feathers kiss your face and flutter everywhere'\n",
        "tokenized_string = target_tokenizer.encode(sample_string)\n",
        "print('Sample string in English: {}'.format(sample_string))\n",
        "print('Sample string tokenized: {}'.format(tokenized_string))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample string in English: Downy feathers kiss your face and flutter everywhere\n",
            "Sample string tokenized: [7899, 1383, 113, 7206, 388, 9, 1519, 1117, 76, 646, 4, 1220, 3165, 7863, 1392]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os5fYTUsxzTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "15221ffc-a12f-47dd-befc-96a103bedd63"
      },
      "source": [
        "sample_string = 'Penas felpudas beijam seu rosto e flutuam por toda parte'\n",
        "tokenized_string = source_tokenizer.encode(sample_string)\n",
        "print('Sample string in Portuguese: {}'.format(sample_string))\n",
        "print('Sample string tokenized: {}'.format(tokenized_string))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample string in Portuguese: Penas felpudas beijam seu rosto e flutuam por toda parte\n",
            "Sample string tokenized: [8038, 2641, 17, 5242, 1174, 43, 3999, 1625, 7990, 109, 5376, 6, 2839, 50, 23, 166, 962]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hQZ65lzyYow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a1c5478-4813-45be-9cd3-3bd9fcec8835"
      },
      "source": [
        "dataloader = DataLoader(source_tokenizer, target_tokenizer)\n",
        "train_dataset = dataloader.get_dataset(train_data, 20000, 64)\n",
        "val_dataset = dataloader.get_dataset(val_data, 20000, 64)\n",
        "print(train_dataset)\n",
        "print(val_dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>\n",
            "<DatasetV1Adapter shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfIG2pgy5Ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34ce492a-011b-4802-d769-a19939e55f9f"
      },
      "source": [
        "source_language_batch, target_language_batch = next(iter(train_dataset))\n",
        "source_language_batch.shape, target_language_batch.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 40]), TensorShape([64, 40]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V5PA4Zv0AUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}